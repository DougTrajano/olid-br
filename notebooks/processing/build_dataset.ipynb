{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLID-BR (Build Dataset)\n",
    "\n",
    "In this notebook, we will build the OLID-BR dataset from the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path(\".\").absolute().parent) not in sys.path:\n",
    "    sys.path.append(str(Path(\".\").absolute().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the env vars\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from src.s3 import Bucket\n",
    "from src.settings import AppSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AppSettings()\n",
    "\n",
    "version = \"v1.0\"\n",
    "\n",
    "bucket = Bucket(args.AWS_S3_BUCKET)\n",
    "\n",
    "bucket.get_session_from_aksk(\n",
    "    args.AWS_ACCESS_KEY_ID,\n",
    "    args.AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "In the next cells, we will load all processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/2/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/2/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/2/full_olidbr.json\"\n",
    "    },\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/3/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/3/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/3/full_olidbr.json\"\n",
    "    },\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/4/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/4/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/4/full_olidbr.json\",\n",
    "        \"additional_data\": \"processed/olid-br/iterations/4/additional_texts.json\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed/olid-br/iterations/2/olidbr.json\n",
      "Data iteration size: 2996\n",
      "Metadata iteration size: 11984\n",
      "Full data iteration size: 2996\n",
      "\n",
      "\n",
      "Loading processed/olid-br/iterations/3/olidbr.json\n",
      "Data iteration size: 2987\n",
      "Metadata iteration size: 11948\n",
      "Full data iteration size: 2987\n",
      "\n",
      "\n",
      "Loading processed/olid-br/iterations/4/olidbr.json\n",
      "Data iteration size: 1995\n",
      "Metadata iteration size: 7980\n",
      "Full data iteration size: 1995\n",
      "Additional data iteration size: 6478\n",
      "\n",
      "\n",
      "Data: 7978\n",
      "Metadata: 31912\n",
      "Full data: 7978\n",
      "Additional data: 6478\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "metadata = []\n",
    "full_data = []\n",
    "additional_data = []\n",
    "\n",
    "for iteration in iterations:\n",
    "    if iteration.get(\"data\"):\n",
    "        print(f\"Loading {iteration['data']}\")\n",
    "        iteration_data = bucket.download_json(key=iteration[\"data\"])\n",
    "        data.extend(iteration_data)\n",
    "        print(f\"Data iteration size: {len(iteration_data)}\")\n",
    "\n",
    "    if iteration.get(\"metadata\"):\n",
    "        iteration_metadata = bucket.download_json(key=iteration[\"metadata\"])\n",
    "        metadata.extend(iteration_metadata)\n",
    "        print(f\"Metadata iteration size: {len(iteration_metadata)}\")\n",
    "\n",
    "    if iteration.get(\"full_data\"):\n",
    "        iteration_full_data = bucket.download_json(key=iteration[\"full_data\"])\n",
    "        full_data.extend(iteration_full_data)\n",
    "        print(f\"Full data iteration size: {len(iteration_full_data)}\")\n",
    "\n",
    "    if iteration.get(\"additional_data\"):\n",
    "        iteration_additional_data = bucket.download_json(key=iteration[\"additional_data\"])\n",
    "        additional_data.extend(iteration_additional_data)\n",
    "        print(f\"Additional data iteration size: {len(iteration_additional_data)}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(f\"Data: {len(data)}\")\n",
    "print(f\"Metadata: {len(metadata)}\")\n",
    "print(f\"Full data: {len(full_data)}\")\n",
    "print(f\"Additional data: {len(additional_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1\n",
    "\n",
    "The first iteration has only the processed data. So, we will need to prepare it specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data iteration 1 size: 706\n",
      "Metadata iteration 1 size: 1520\n"
     ]
    }
   ],
   "source": [
    "iteration_1 = {\n",
    "    \"data\": \"processed/olid-br/iterations/1/olidbr.json\",\n",
    "    \"metadata\": \"processed/olid-br/iterations/1/metadata.json\"\n",
    "}\n",
    "\n",
    "iteration_1_data = bucket.download_json(key=iteration_1[\"data\"])\n",
    "iteration_1_metadata = bucket.download_json(key=iteration_1[\"metadata\"])\n",
    "\n",
    "print(f\"Data iteration 1 size: {len(iteration_1_data)}\")\n",
    "print(f\"Metadata iteration 1 size: {len(iteration_1_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional data: 7184\n"
     ]
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(iteration_1_metadata)\n",
    "\n",
    "for i in iteration_1_data:\n",
    "    item = {\n",
    "        \"id\": i[\"id\"],\n",
    "        \"text\": i[\"text\"],\n",
    "        \"metadata\": {},\n",
    "        \"annotations\": []\n",
    "    }\n",
    "\n",
    "    # Get the metadata\n",
    "    metadata = df_metadata[\n",
    "        df_metadata[\"id\"] == i[\"id\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    for m in metadata:\n",
    "        annotators = df_metadata[df_metadata[\"id\"] == i[\"id\"]][\"annotator_id\"].unique()\n",
    "        if 1.0 in annotators:\n",
    "            main_annotator = 1.0\n",
    "        elif 32.0 in annotators:\n",
    "            main_annotator = 32.0\n",
    "        else:\n",
    "            ValueError(\n",
    "                f\"Annotator not found for {i['id']} - Annotators: {annotators}\")\n",
    "        \n",
    "        if not pd.isnull(m[\"source\"]):\n",
    "            for key in [\"source\",\n",
    "                        \"created_at\",\n",
    "                        \"collected_at\",\n",
    "                        \"toxicity_score\"]:\n",
    "                item[\"metadata\"][key] = m[key]\n",
    "\n",
    "        elif (\n",
    "                not pd.isnull(m[\"annotator_id\"])\n",
    "                and m[\"annotator_id\"] == main_annotator):\n",
    "            annotations = {\n",
    "                \"annotator_id\": m[\"annotator_id\"],\n",
    "            }\n",
    "\n",
    "            for key in [\"is_offensive\",\n",
    "                        \"is_targeted\",\n",
    "                        \"targeted_type\",\n",
    "                        \"toxic_spans\",\n",
    "                        \"health\",\n",
    "                        \"ideology\",\n",
    "                        \"insult\",\n",
    "                        \"lgbtqphobia\",\n",
    "                        \"other_lifestyle\",\n",
    "                        \"physical_aspects\",\n",
    "                        \"profanity_obscene\",\n",
    "                        \"racism\",\n",
    "                        \"religious_intolerance\",\n",
    "                        \"sexism\",\n",
    "                        \"xenophobia\"]:\n",
    "                annotations[key] = i[key]\n",
    "\n",
    "            item[\"annotations\"].append(annotations)\n",
    "\n",
    "            assert len(annotations) == 16\n",
    "            assert len(item[\"annotations\"]) == 1\n",
    "            assert len(item[\"metadata\"]) == 4\n",
    "            \n",
    "    additional_data.append(item)\n",
    "\n",
    "print(f\"Additional data: {len(additional_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "In this section, we will perform some data processing in order to clean and fix some issues in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated text: 35\n",
      "(7943, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(f\"Duplicated text: {df['text'].duplicated().sum()}\")\n",
    "\n",
    "df.drop_duplicates(subset=\"text\", inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "data = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: 7943\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated texts from full data\n",
    "full_data = [i for i in full_data if i[\"id\"] in df[\"id\"].values]\n",
    "\n",
    "print(f\"Full data: {len(full_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count metadata (before): 2\n",
      "Count metadata (after): 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated texts from metadata\n",
    "print(f\"Count metadata (before): {len(metadata)}\")\n",
    "metadata = [i for i in metadata if i[\"id\"] in df[\"id\"].values]\n",
    "print(f\"Count metadata (after): {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Report\n",
    "\n",
    "In this section, we will generate a profiling report for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dd696cb6224cf69842bcf0770f2975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f7cd4a9df144c2b9c10f12a3e43d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c506807faec442be88654736debd7fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147632de8e4c4fa59f7f11651dccf569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(\n",
    "    pd.DataFrame(data),\n",
    "    title=f\"OLID-BR {version}\",\n",
    "    explorative=True)\n",
    "\n",
    "profile.to_file(f\"../../docs/reports/olidbr_{version}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, validation and test sets\n",
    "\n",
    "- **Training set**: 60% of the dataset.\n",
    "- **Validation set**: 20% of the dataset.\n",
    "- **Test set**: 20% of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `is_offensive`, `is_targeted`, and `targeted_type` categories values to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_offensive\n",
    "df[\"is_offensive\"] = df[\"is_offensive\"].replace({\n",
    "    \"OFF\": 1,\n",
    "    \"NOT\": 0\n",
    "})\n",
    "\n",
    "# is_targeted\n",
    "df[\"is_targeted\"] = df[\"is_targeted\"].replace({\n",
    "    \"TIN\": 1,\n",
    "    \"UNT\": 0\n",
    "})\n",
    "\n",
    "# targeted_type\n",
    "df[\"targeted_type\"].fillna(0, inplace=True)\n",
    "df[\"targeted_type\"] = df[\"targeted_type\"].replace({\n",
    "    \"IND\": 1,\n",
    "    \"GRP\": 2,\n",
    "    \"OTH\": 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"seed\": 1993,\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.25 # 0.25 x 0.8 = 0.2\n",
    "}\n",
    "\n",
    "labels = [\n",
    "    \"is_offensive\",\n",
    "    \"is_targeted\",\n",
    "    \"targeted_type\",\n",
    "    \"health\",\n",
    "    \"ideology\",\n",
    "    \"insult\",\n",
    "    \"lgbtqphobia\",\n",
    "    \"other_lifestyle\",\n",
    "    \"physical_aspects\",\n",
    "    \"profanity_obscene\",\n",
    "    \"racism\",\n",
    "    \"religious_intolerance\",\n",
    "    \"sexism\",\n",
    "    \"xenophobia\"\n",
    "]\n",
    "\n",
    "X = df[[\"id\", \"text\"]].values\n",
    "y = df[labels].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4765, 2)\n",
      "Validation: (1589, 2)\n",
      "Test: (1589, 2)\n"
     ]
    }
   ],
   "source": [
    "from src.modeling.selection import multilabel_train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    X, y, test_size=params[\"test_size\"],\n",
    "    random_state=params[\"seed\"], stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = multilabel_train_test_split(\n",
    "    X_train, y_train, test_size=params[\"val_size\"],\n",
    "    random_state=params[\"seed\"], stratify=y_train)\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4765\n",
      "Validation: 1589\n",
      "Test: 1589\n",
      "Full train: 4765\n",
      "Full validation: 1589\n",
      "Full test: 1589\n"
     ]
    }
   ],
   "source": [
    "train_data = [item for item in data if item[\"id\"] in X_train[:, 0]]\n",
    "val_data = [item for item in data if item[\"id\"] in X_val[:, 0]]\n",
    "test_data = [item for item in data if item[\"id\"] in X_test[:, 0]]\n",
    "\n",
    "train_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in train_data]]\n",
    "val_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in val_data]]\n",
    "test_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in test_data]]\n",
    "\n",
    "full_train_data = [item for item in full_data if item[\"id\"] in X_train[:, 0]]\n",
    "full_val_data = [item for item in full_data if item[\"id\"] in X_val[:, 0]]\n",
    "full_test_data = [item for item in full_data if item[\"id\"] in X_test[:, 0]]\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "print(f\"Full train: {len(full_train_data)}\")\n",
    "print(f\"Full validation: {len(full_val_data)}\")\n",
    "print(f\"Full test: {len(full_test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(full_data) == len(train_data) + len(val_data) + len(test_data)\n",
    "\n",
    "assert len(metadata) == len(train_metadata) + len(val_metadata) + len(test_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train.csv - Done\n",
      "Saving validation.csv - Done\n",
      "Saving test.csv - Done\n",
      "Saving train_metadata.csv - Done\n",
      "Saving validation_metadata.csv - Done\n",
      "Saving test_metadata.csv - Done\n",
      "Saving train.json - Done\n",
      "Saving validation.json - Done\n",
      "Saving test.json - Done\n",
      "Saving additional_data.json - Done\n",
      "All files uploaded.\n"
     ]
    }
   ],
   "source": [
    "files = {\n",
    "    \"train.csv\": pd.DataFrame(train_data),\n",
    "    \"validation.csv\": pd.DataFrame(val_data),\n",
    "    \"test.csv\": pd.DataFrame(test_data),\n",
    "    \"train_metadata.csv\": pd.DataFrame(train_metadata),\n",
    "    \"validation_metadata.csv\": pd.DataFrame(val_metadata),\n",
    "    \"test_metadata.csv\": pd.DataFrame(test_metadata),\n",
    "    \"train.json\": full_train_data,\n",
    "    \"validation.json\": full_val_data,\n",
    "    \"test.json\": full_test_data,\n",
    "    \"additional_data.json\": additional_data\n",
    "}\n",
    "\n",
    "for file, obj in files.items():\n",
    "    print(f\"Saving {file}\", end=\"\")\n",
    "    if file.endswith(\".csv\"):\n",
    "        bucket.upload_csv(\n",
    "            data=obj,\n",
    "            key=f\"processed/olid-br/{version}/{file}\")\n",
    "    elif file.endswith(\".json\"):\n",
    "        bucket.upload_json(\n",
    "            data=obj,\n",
    "            key=f\"processed/olid-br/{version}/{file}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format\")\n",
    "    print(\" - Done\")\n",
    "\n",
    "print(\"All files uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to Kaggle and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_files = [\n",
    "    \"validation.csv\",\n",
    "    \"validation_metadata.csv\",\n",
    "    \"validation.json\"\n",
    "]\n",
    "\n",
    "dataset_metadata = {\n",
    "    \"id\": \"dougtrajano/olidbr\",\n",
    "    \"licenses\": [{\"name\": \"CC BY 4.0\"}],\n",
    "    \"title\": \"OLID-BR\"\n",
    "}\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "if not os.path.exists(f\"{temp_dir}/dataset-metadata.json\"):\n",
    "    with open(os.path.join(temp_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "for file, obj in files.items():\n",
    "    if file.endswith(\".csv\") and file not in private_files:\n",
    "        obj.to_csv(f\"{temp_dir}/{file}\", index=False, encoding=\"utf-8\")\n",
    "    elif file.endswith(\".json\") and file not in private_files:\n",
    "        with open(f\"{temp_dir}/{file}\", \"w\") as f:\n",
    "            json.dump(obj, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file additional_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.1M/11.1M [00:02<00:00, 4.12MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: additional_data.json (11MB)\n",
      "Starting upload for file test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500k/500k [00:01<00:00, 263kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test.csv (500KB)\n",
      "Starting upload for file test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.91M/4.91M [00:02<00:00, 2.12MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test.json (5MB)\n",
      "Starting upload for file test_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00/2.00 [00:02<00:00, 1.03s/B]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test_metadata.csv (2B)\n",
      "Starting upload for file train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.47M/1.47M [00:02<00:00, 759kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.csv (1MB)\n",
      "Starting upload for file train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14.7M/14.7M [00:02<00:00, 5.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.json (15MB)\n",
      "Starting upload for file train_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00/2.00 [00:02<00:00, 1.04s/B]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train_metadata.csv (2B)\n",
      "Dataset uploaded to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_create_version(\n",
    "    folder=temp_dir,\n",
    "    version_notes=f\"OLID-BR {version} - {date}\",\n",
    "    convert_to_csv=False,\n",
    "    delete_old_versions=True\n",
    ")\n",
    "\n",
    "print(\"Dataset uploaded to Kaggle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia'],\n",
       "        num_rows: 4765\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia'],\n",
       "        num_rows: 1589\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.dataset_dict.DatasetDict({\n",
    "    \"train\": datasets.Dataset.from_pandas(pd.DataFrame(train_data), split=\"train\"),\n",
    "    \"test\": datasets.Dataset.from_pandas(pd.DataFrame(test_data), split=\"test\"),\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524e4258ba9e49789a6107cae956b5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "BadRequestError() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:187\u001b[0m, in \u001b[0;36m_raise_convert_bad_request\u001b[1;34m(resp, endpoint_name)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     _raise_for_status(resp)\n\u001b[0;32m    188\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:169\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    167\u001b[0m _add_server_message_to_error_args(e, response)\n\u001b[1;32m--> 169\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:131\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    132\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:1022\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1022\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/dougtrajano/olid-br/commit/main (Request ID: ldZPk08-WObQbweziMgP-)\n\nError when retrieving LFS file data/train-00000-of-00001-f8bacdb582f910e4.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\trajano\\OneDrive - HP Inc\\Doug\\GitHub\\pucrs\\olid-br\\notebooks\\processing\\build_dataset.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/olid-br/notebooks/processing/build_dataset.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset\u001b[39m.\u001b[39;49mpush_to_hub(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/olid-br/notebooks/processing/build_dataset.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     repo_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdougtrajano/olid-br\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/olid-br/notebooks/processing/build_dataset.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     private\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/trajano/OneDrive%20-%20HP%20Inc/Doug/GitHub/pucrs/olid-br/notebooks/processing/build_dataset.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     token\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mHUGGINGFACE_HUB_TOKEN)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\dataset_dict.py:1356\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[1;34m(self, repo_id, private, token, branch, max_shard_size, shard_size, embed_external_files)\u001b[0m\n\u001b[0;32m   1354\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPushing split \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1355\u001b[0m \u001b[39m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[1;32m-> 1356\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[split]\u001b[39m.\u001b[39;49m_push_parquet_shards_to_hub(\n\u001b[0;32m   1357\u001b[0m     repo_id,\n\u001b[0;32m   1358\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[0;32m   1359\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[0;32m   1360\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1361\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[0;32m   1362\u001b[0m     max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size,\n\u001b[0;32m   1363\u001b[0m     embed_external_files\u001b[39m=\u001b[39;49membed_external_files,\n\u001b[0;32m   1364\u001b[0m )\n\u001b[0;32m   1365\u001b[0m total_uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m uploaded_size\n\u001b[0;32m   1366\u001b[0m total_dataset_nbytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dataset_nbytes\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:4275\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[1;34m(self, repo_id, split, private, token, branch, max_shard_size, embed_external_files)\u001b[0m\n\u001b[0;32m   4273\u001b[0m         shard\u001b[39m.\u001b[39mto_parquet(buffer)\n\u001b[0;32m   4274\u001b[0m         uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mtell()\n\u001b[1;32m-> 4275\u001b[0m         _retry(\n\u001b[0;32m   4276\u001b[0m             api\u001b[39m.\u001b[39;49mupload_file,\n\u001b[0;32m   4277\u001b[0m             func_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(\n\u001b[0;32m   4278\u001b[0m                 path_or_fileobj\u001b[39m=\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mgetvalue(),\n\u001b[0;32m   4279\u001b[0m                 path_in_repo\u001b[39m=\u001b[39;49mshard_path_in_repo,\n\u001b[0;32m   4280\u001b[0m                 repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[0;32m   4281\u001b[0m                 token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   4282\u001b[0m                 repo_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   4283\u001b[0m                 revision\u001b[39m=\u001b[39;49mbranch,\n\u001b[0;32m   4284\u001b[0m             ),\n\u001b[0;32m   4285\u001b[0m             exceptions\u001b[39m=\u001b[39;49mHTTPError,\n\u001b[0;32m   4286\u001b[0m             status_codes\u001b[39m=\u001b[39;49m[\u001b[39m504\u001b[39;49m],\n\u001b[0;32m   4287\u001b[0m             base_wait_time\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m,\n\u001b[0;32m   4288\u001b[0m             max_retries\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m   4289\u001b[0m             max_wait_time\u001b[39m=\u001b[39;49m\u001b[39m20.0\u001b[39;49m,\n\u001b[0;32m   4290\u001b[0m         )\n\u001b[0;32m   4291\u001b[0m     shards_path_in_repo\u001b[39m.\u001b[39mappend(shard_path_in_repo)\n\u001b[0;32m   4293\u001b[0m \u001b[39m# Cleanup to remove unused files\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:285\u001b[0m, in \u001b[0;36m_retry\u001b[1;34m(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m    286\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m retry \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_retries \u001b[39mor\u001b[39;00m (status_codes \u001b[39mand\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m status_codes):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\hf_api.py:2184\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[1;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, identical_ok, commit_message, commit_description, create_pr, parent_commit)\u001b[0m\n\u001b[0;32m   2174\u001b[0m commit_message \u001b[39m=\u001b[39m (\n\u001b[0;32m   2175\u001b[0m     commit_message\n\u001b[0;32m   2176\u001b[0m     \u001b[39mif\u001b[39;00m commit_message \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpload \u001b[39m\u001b[39m{\u001b[39;00mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m with huggingface_hub\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2178\u001b[0m )\n\u001b[0;32m   2179\u001b[0m operation \u001b[39m=\u001b[39m CommitOperationAdd(\n\u001b[0;32m   2180\u001b[0m     path_or_fileobj\u001b[39m=\u001b[39mpath_or_fileobj,\n\u001b[0;32m   2181\u001b[0m     path_in_repo\u001b[39m=\u001b[39mpath_in_repo,\n\u001b[0;32m   2182\u001b[0m )\n\u001b[1;32m-> 2184\u001b[0m pr_url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_commit(\n\u001b[0;32m   2185\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[0;32m   2186\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m   2187\u001b[0m     operations\u001b[39m=\u001b[39;49m[operation],\n\u001b[0;32m   2188\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[0;32m   2189\u001b[0m     commit_description\u001b[39m=\u001b[39;49mcommit_description,\n\u001b[0;32m   2190\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   2191\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   2192\u001b[0m     create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[0;32m   2193\u001b[0m     parent_commit\u001b[39m=\u001b[39;49mparent_commit,\n\u001b[0;32m   2194\u001b[0m )\n\u001b[0;32m   2196\u001b[0m \u001b[39mif\u001b[39;00m pr_url \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2197\u001b[0m     revision \u001b[39m=\u001b[39m quote(_parse_revision_from_pr_url(pr_url), safe\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\hf_api.py:2051\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[1;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit)\u001b[0m\n\u001b[0;32m   2043\u001b[0m commit_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/api/\u001b[39m\u001b[39m{\u001b[39;00mrepo_type\u001b[39m}\u001b[39;00m\u001b[39ms/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m/commit/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2045\u001b[0m commit_resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[0;32m   2046\u001b[0m     url\u001b[39m=\u001b[39mcommit_url,\n\u001b[0;32m   2047\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[0;32m   2048\u001b[0m     json\u001b[39m=\u001b[39mcommit_payload,\n\u001b[0;32m   2049\u001b[0m     params\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcreate_pr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m} \u001b[39mif\u001b[39;00m create_pr \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2050\u001b[0m )\n\u001b[1;32m-> 2051\u001b[0m _raise_convert_bad_request(commit_resp, endpoint_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcommit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2052\u001b[0m \u001b[39mreturn\u001b[39;00m commit_resp\u001b[39m.\u001b[39mjson()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mpullRequestUrl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:195\u001b[0m, in \u001b[0;36m_raise_convert_bad_request\u001b[1;34m(resp, endpoint_name)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m \u001b[39mand\u001b[39;00m details:\n\u001b[1;32m--> 195\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(\n\u001b[0;32m    196\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBad request for \u001b[39;49m\u001b[39m{\u001b[39;49;00mendpoint_name\u001b[39m}\u001b[39;49;00m\u001b[39m endpoint: \u001b[39;49m\u001b[39m{\u001b[39;49;00mdetails\u001b[39m}\u001b[39;49;00m\u001b[39m (Request ID:\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    197\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00mrequest_id\u001b[39m}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    198\u001b[0m         response\u001b[39m=\u001b[39;49mresp,\n\u001b[0;32m    199\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m    200\u001b[0m _add_request_id_to_error_args(exc, request_id\u001b[39m=\u001b[39mrequest_id)\n\u001b[0;32m    201\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:96\u001b[0m, in \u001b[0;36mBadRequestError.__init__\u001b[1;34m(self, message, response)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, message, response):\n\u001b[1;32m---> 96\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(message, response\u001b[39m=\u001b[39;49mresponse)\n",
      "\u001b[1;31mTypeError\u001b[0m: BadRequestError() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(\n",
    "    repo_id=\"dougtrajano/olid-br\",\n",
    "    private=True,\n",
    "    token=args.HUGGINGFACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Rater Reliability (IRR) analysis\n",
    "\n",
    "a.k.a inter-rater agreement (IRA) or concordance.\n",
    "\n",
    "In the next cells, we will perform an agreement analysis to check if the annotations are consistent.\n",
    "\n",
    "See [Inter-Rater Reliability - OLID-BR](https://dougtrajano.github.io/olid-br/annotation/inter-rater-reliability.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from irrCAC.raw import CAC\n",
    "from src.labeling.metrics import percent_agreement\n",
    "from src.utils import get_annotations_by_rater\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics import agreement\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import masi_distance, jaccard_distance\n",
    "\n",
    "def get_annotations(\n",
    "    annotations: Dict[str, Any],\n",
    "    label: str,\n",
    "    annotation_id_key: str = \"annotator_id\") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Get annotations for a given label.\n",
    "\n",
    "    Args:\n",
    "    - annotations: Dictionary with annotations.\n",
    "    - label: Label to get annotations for.\n",
    "    - annotation_id_key: Key to use for annotation id.\n",
    "\n",
    "    Returns:\n",
    "    - List of annotations for the given label.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for annotation in annotations:\n",
    "        for key, value in annotation.items():\n",
    "            if key == label:\n",
    "                data[annotation[annotation_id_key]] = value\n",
    "    \n",
    "    # Replace annotator ids to A, B, C, etc.\n",
    "    annotators = {\n",
    "        126: \"A\",\n",
    "        127: \"B\",\n",
    "        128: \"C\",\n",
    "        260: \"C\",\n",
    "        504: \"A\",\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        annotators.get(key, key): value\n",
    "        for key, value in data.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `is_offensive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_offensive = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"is_offensive\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "is_offensive.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(is_offensive)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `is_targeted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_targeted = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"is_targeted\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "is_targeted.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(is_targeted)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `targeted_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_type = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"targeted_type\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "targeted_type.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(targeted_type)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `toxic_spans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_spans = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"toxic_spans\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "toxic_spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = []\n",
    "for annotator in toxic_spans.columns:\n",
    "    for item in range(len(toxic_spans)):\n",
    "        temp = toxic_spans.iloc[item][annotator]\n",
    "        if temp != []:\n",
    "            task_data.append((\n",
    "                annotator,\n",
    "                item,\n",
    "                frozenset(temp)\n",
    "            ))\n",
    "\n",
    "jaccard_task = AnnotationTask(distance=jaccard_distance)\n",
    "masi_task = AnnotationTask(distance=masi_distance)\n",
    "\n",
    "for task in [jaccard_task, masi_task]:\n",
    "    task.load_array(task_data)\n",
    "    print(f\"Krippendorff's alpha using {task.distance}\")\n",
    "    print(f\"Krippendorff's alpha: {task.alpha():.4f}\", \"\\n\")\n",
    "\n",
    "print(f\"Percent agreement: {percent_agreement(toxic_spans):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `health`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"health\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "health.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(health)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ideology`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideology = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"ideology\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "ideology.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(ideology)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `insult`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insult = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"insult\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "insult.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(insult)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lgbtqphobia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbtqphobia = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"lgbtqphobia\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "lgbtqphobia.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(lgbtqphobia)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `other_lifestyle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_lifestyle = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"other_lifestyle\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "other_lifestyle.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(other_lifestyle)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `physical_aspects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_aspects = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"physical_aspects\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "physical_aspects.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(physical_aspects)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `profanity_obscene`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity_obscene = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"profanity_obscene\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "profanity_obscene.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(profanity_obscene)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `racism`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racism = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"racism\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "racism.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(racism)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `religious_intolerance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religious_intolerance = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"religious_intolerance\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "religious_intolerance.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(religious_intolerance)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sexism`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexism = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"sexism\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "sexism.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(sexism)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `xenophobia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenophobia = pd.DataFrame(\n",
    "    [\n",
    "        get_annotations(\n",
    "            annotations=item[\"annotations\"],\n",
    "            label=\"xenophobia\",\n",
    "            annotation_id_key=\"annotator_id\")\n",
    "        for item in full_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "xenophobia.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac = CAC(xenophobia)\n",
    "\n",
    "print(\"CAC:\", cac)\n",
    "print(f\"Percent agreement: {percent_agreement(cac.ratings):.4f}\")\n",
    "print(f\"Krippendorff's alpha: {cac.krippendorff()['est']['coefficient_value']:.4f}\")\n",
    "print(f\"Gwet's AC1: {cac.gwet()['est']['coefficient_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krispendorff's alpha Multi-Label\n",
    "\n",
    "In the next cells, we will calculate the Krippendorff's alpha considering as a multi-label problem instead of several binary problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = {\n",
    "    \"health\": health,\n",
    "    \"ideology\": ideology,\n",
    "    \"insult\": insult,\n",
    "    \"lgbtqphobia\": lgbtqphobia,\n",
    "    \"other_lifestyle\": other_lifestyle,\n",
    "    \"physical_aspects\": physical_aspects,\n",
    "    \"profanity_obscene\": profanity_obscene,\n",
    "    \"racism\": racism,\n",
    "    \"religious_intolerance\": religious_intolerance,\n",
    "    \"sexism\": sexism,\n",
    "    \"xenophobia\": xenophobia\n",
    "}\n",
    "\n",
    "task_data = []\n",
    "for annotator in health.columns.tolist():\n",
    "    for item in range(len(health)):\n",
    "        temp = get_annotations_by_rater(ratings, annotator, item)\n",
    "        if temp != []:\n",
    "            task_data.append((\n",
    "                annotator,\n",
    "                item,\n",
    "                frozenset(temp)\n",
    "            ))\n",
    "\n",
    "jaccard_task = AnnotationTask(distance=jaccard_distance)\n",
    "masi_task = AnnotationTask(distance=masi_distance)\n",
    "\n",
    "for task in [jaccard_task, masi_task]:\n",
    "    task.load_array(task_data)\n",
    "    print(f\"Krippendorff's alpha using {task.distance}\")\n",
    "    print(f\"Krippendorff's alpha: {task.alpha():.4f}\", \"\\n\")\n",
    "\n",
    "pa_mlabels = {}\n",
    "for item in range(len(health)):\n",
    "    for annotator in health.columns.tolist():\n",
    "        temp = get_annotations_by_rater(ratings, annotator, item)\n",
    "        \n",
    "        if annotator not in pa_mlabels.keys():\n",
    "            pa_mlabels[annotator] = []\n",
    "        \n",
    "        pa_mlabels[annotator].append(temp)\n",
    "\n",
    "print(f\"Percent agreement: {percent_agreement(pd.DataFrame(pa_mlabels)):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
