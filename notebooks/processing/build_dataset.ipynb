{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLID-BR (Build Dataset)\n",
    "\n",
    "In this notebook, we will build the OLID-BR dataset from the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path(\".\").absolute().parent) not in sys.path:\n",
    "    sys.path.append(str(Path(\".\").absolute().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the env vars\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from src.s3 import Bucket\n",
    "from src.settings import AppSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AppSettings()\n",
    "\n",
    "version = \"v0.4-alpha\"\n",
    "\n",
    "bucket = Bucket(args.AWS_S3_BUCKET)\n",
    "\n",
    "bucket.get_session_from_aksk(\n",
    "    args.AWS_ACCESS_KEY_ID,\n",
    "    args.AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "In the next cells, we will load all processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [\n",
    "    # {\n",
    "    #     \"data\": \"processed/olid-br/iterations/1/olidbr.json\",\n",
    "    #     \"metadata\": \"processed/olid-br/iterations/1/metadata.json\"\n",
    "    # },\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/2/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/2/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/2/full_olidbr.json\"\n",
    "    },\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/3/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/3/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/3/full_olidbr.json\"\n",
    "    },\n",
    "    {\n",
    "        \"data\": \"processed/olid-br/iterations/4/olidbr.json\",\n",
    "        \"metadata\": \"processed/olid-br/iterations/4/metadata.json\",\n",
    "        \"full_data\": \"processed/olid-br/iterations/4/full_olidbr.json\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed/olid-br/iterations/2/olidbr.json\n",
      "Data iteration size: 2996\n",
      "Metadata iteration size: 11984\n",
      "Full data iteration size: 2996\n",
      "Loading processed/olid-br/iterations/3/olidbr.json\n",
      "Data iteration size: 2987\n",
      "Metadata iteration size: 11948\n",
      "Full data iteration size: 2987\n",
      "Loading processed/olid-br/iterations/4/olidbr.json\n",
      "Data iteration size: 1851\n",
      "Metadata iteration size: 7404\n",
      "Full data iteration size: 1851\n",
      "Data: 7834\n",
      "Metadata: 31336\n",
      "Full data: 7834\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "metadata = []\n",
    "full_data = []\n",
    "\n",
    "for iteration in iterations:\n",
    "    print(f\"Loading {iteration['data']}\")\n",
    "\n",
    "    iteration_data = bucket.download_json(key=iteration[\"data\"])\n",
    "    iteration_metadata = bucket.download_json(key=iteration[\"metadata\"])\n",
    "\n",
    "    data.extend(iteration_data)\n",
    "    metadata.extend(iteration_metadata)\n",
    "    \n",
    "    print(f\"Data iteration size: {len(iteration_data)}\")\n",
    "    print(f\"Metadata iteration size: {len(iteration_metadata)}\")\n",
    "\n",
    "    if iteration.get(\"full_data\"):\n",
    "        iteration_full_data = bucket.download_json(key=iteration[\"full_data\"])\n",
    "        full_data.extend(iteration_full_data)\n",
    "        print(f\"Full data iteration size: {len(iteration_full_data)}\")\n",
    "\n",
    "print(f\"Data: {len(data)}\")\n",
    "print(f\"Metadata: {len(metadata)}\")\n",
    "print(f\"Full data: {len(full_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "In this section, we will perform some data processing in order to clean and fix some issues in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated text: 34\n",
      "(7800, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(f\"Duplicated text: {df['text'].duplicated().sum()}\")\n",
    "\n",
    "df.drop_duplicates(subset=\"text\", inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "data = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: 7834\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated texts from full data\n",
    "full_data = [i for i in full_data if i[\"text\"] in df[\"text\"].values]\n",
    "\n",
    "print(f\"Full data: {len(full_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count metadata (before): 31336\n",
      "Count metadata (after): 31200\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated texts from metadata\n",
    "print(f\"Count metadata (before): {len(metadata)}\")\n",
    "metadata = [i for i in metadata if i[\"id\"] in df[\"id\"].values]\n",
    "print(f\"Count metadata (after): {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Report\n",
    "\n",
    "In this section, we will generate a profiling report for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2520ec553445d195dfe1e4e3f8b455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf0d78e2fbd41548cebff8923cd1e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcb7c2abd9145a5adcc0391928eba16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb2142ddde545f18c2a9044f5ed58be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(\n",
    "    pd.DataFrame(data),\n",
    "    title=f\"OLID-BR {version}\",\n",
    "    explorative=True)\n",
    "\n",
    "profile.to_file(f\"../../docs/reports/olidbr_{version}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, validation and test sets\n",
    "\n",
    "- **Training set**: 60% of the dataset.\n",
    "- **Validation set**: 20% of the dataset.\n",
    "- **Test set**: 20% of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `is_offensive`, `is_targeted`, and `targeted_type` categories values to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_offensive\n",
    "df[\"is_offensive\"] = df[\"is_offensive\"].replace({\n",
    "    \"OFF\": 1,\n",
    "    \"NOT\": 0\n",
    "})\n",
    "\n",
    "# is_targeted\n",
    "df[\"is_targeted\"] = df[\"is_targeted\"].replace({\n",
    "    \"TIN\": 1,\n",
    "    \"UNT\": 0\n",
    "})\n",
    "\n",
    "# targeted_type\n",
    "df[\"targeted_type\"].fillna(0, inplace=True)\n",
    "df[\"targeted_type\"] = df[\"targeted_type\"].replace({\n",
    "    \"IND\": 1,\n",
    "    \"GRP\": 2,\n",
    "    \"OTH\": 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"seed\": 1993,\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.25 # 0.25 x 0.8 = 0.2\n",
    "}\n",
    "\n",
    "labels = [\n",
    "    \"is_offensive\",\n",
    "    \"is_targeted\",\n",
    "    \"targeted_type\",\n",
    "    \"health\",\n",
    "    \"ideology\",\n",
    "    \"insult\",\n",
    "    \"lgbtqphobia\",\n",
    "    \"other_lifestyle\",\n",
    "    \"physical_aspects\",\n",
    "    \"profanity_obscene\",\n",
    "    \"racism\",\n",
    "    \"sexism\",\n",
    "    \"xenophobia\"\n",
    "]\n",
    "\n",
    "X = df[[\"id\", \"text\"]].values\n",
    "y = df[labels].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4680, 2)\n",
      "Validation: (1560, 2)\n",
      "Test: (1560, 2)\n"
     ]
    }
   ],
   "source": [
    "from src.modeling.selection import multilabel_train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    X, y, test_size=params[\"test_size\"],\n",
    "    random_state=params[\"seed\"], stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = multilabel_train_test_split(\n",
    "    X_train, y_train, test_size=params[\"val_size\"],\n",
    "    random_state=params[\"seed\"], stratify=y_train)\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4680\n",
      "Validation: 1560\n",
      "Test: 1560\n",
      "Full train: 4680\n",
      "Full validation: 1560\n",
      "Full test: 1560\n"
     ]
    }
   ],
   "source": [
    "train_data = [item for item in data if item[\"id\"] in X_train[:, 0]]\n",
    "val_data = [item for item in data if item[\"id\"] in X_val[:, 0]]\n",
    "test_data = [item for item in data if item[\"id\"] in X_test[:, 0]]\n",
    "\n",
    "train_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in train_data]]\n",
    "val_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in val_data]]\n",
    "test_metadata = [item for item in metadata if item[\"id\"] in [i[\"id\"] for i in test_data]]\n",
    "\n",
    "full_train_data = [item for item in full_data if item[\"id\"] in X_train[:, 0]]\n",
    "full_val_data = [item for item in full_data if item[\"id\"] in X_val[:, 0]]\n",
    "full_test_data = [item for item in full_data if item[\"id\"] in X_test[:, 0]]\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "print(f\"Full train: {len(full_train_data)}\")\n",
    "print(f\"Full validation: {len(full_val_data)}\")\n",
    "print(f\"Full test: {len(full_test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train.csv - Done\n",
      "Saving validation.csv - Done\n",
      "Saving test.csv - Done\n",
      "Saving train_metatada.csv - Done\n",
      "Saving validation_metatada.csv - Done\n",
      "Saving test_metatada.csv - Done\n",
      "Saving train.json - Done\n",
      "Saving validation.json - Done\n",
      "Saving test.json - Done\n",
      "All files uploaded.\n"
     ]
    }
   ],
   "source": [
    "files = {\n",
    "    \"train.csv\": pd.DataFrame(train_data),\n",
    "    \"validation.csv\": pd.DataFrame(val_data),\n",
    "    \"test.csv\": pd.DataFrame(test_data),\n",
    "    \"train_metatada.csv\": pd.DataFrame(train_metadata),\n",
    "    \"validation_metatada.csv\": pd.DataFrame(val_metadata),\n",
    "    \"test_metatada.csv\": pd.DataFrame(test_metadata),\n",
    "    \"train.json\": full_train_data,\n",
    "    \"validation.json\": full_val_data,\n",
    "    \"test.json\": full_test_data\n",
    "}\n",
    "\n",
    "for file, obj in files.items():\n",
    "    print(f\"Saving {file}\", end=\"\")\n",
    "    if file.endswith(\".csv\"):\n",
    "        bucket.upload_csv(\n",
    "            data=obj,\n",
    "            key=f\"processed/olid-br/{version}/{file}\")\n",
    "    elif file.endswith(\".json\"):\n",
    "        bucket.upload_json(\n",
    "            data=obj,\n",
    "            key=f\"processed/olid-br/{version}/{file}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format\")\n",
    "    print(\" - Done\")\n",
    "\n",
    "print(\"All files uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to Kaggle and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_files = [\n",
    "    \"validation.csv\",\n",
    "    \"validation_metatada.csv\",\n",
    "    \"validation.json\"\n",
    "]\n",
    "\n",
    "dataset_metadata = {\n",
    "    \"id\": \"dougtrajano/olidbr\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}],\n",
    "    \"title\": \"OLID-BR\"\n",
    "}\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "if not os.path.exists(f\"{temp_dir}/dataset-metadata.json\"):\n",
    "    with open(os.path.join(temp_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "for file, obj in files.items():\n",
    "    if file.endswith(\".csv\") and file not in private_files:\n",
    "        obj.to_csv(f\"{temp_dir}/{file}\", index=False, encoding=\"utf-8\")\n",
    "    elif file.endswith(\".json\") and file not in private_files:\n",
    "        with open(f\"{temp_dir}/{file}\", \"w\") as f:\n",
    "            json.dump(obj, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493k/493k [00:02<00:00, 171kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test.csv (493KB)\n",
      "Starting upload for file test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.78M/4.78M [00:02<00:00, 1.81MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test.json (5MB)\n",
      "Starting upload for file test_metatada.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 590k/590k [00:02<00:00, 273kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: test_metatada.csv (590KB)\n",
      "Starting upload for file train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.45M/1.45M [00:02<00:00, 633kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.csv (1MB)\n",
      "Starting upload for file train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14.6M/14.6M [00:03<00:00, 5.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.json (15MB)\n",
      "Starting upload for file train_metatada.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.73M/1.73M [00:02<00:00, 694kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train_metatada.csv (2MB)\n",
      "Dataset uploaded to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_create_version(\n",
    "    folder=temp_dir,\n",
    "    version_notes=f\"OLID-BR {version} - {date}\",\n",
    "    convert_to_csv=False\n",
    ")\n",
    "\n",
    "print(\"Dataset uploaded to Kaggle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia'],\n",
       "        num_rows: 4680\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'is_offensive', 'is_targeted', 'targeted_type', 'toxic_spans', 'health', 'ideology', 'insult', 'lgbtqphobia', 'other_lifestyle', 'physical_aspects', 'profanity_obscene', 'racism', 'religious_intolerance', 'sexism', 'xenophobia'],\n",
       "        num_rows: 1560\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.dataset_dict.DatasetDict({\n",
    "    \"train\": datasets.Dataset.from_pandas(pd.DataFrame(train_data), split=\"train\"),\n",
    "    \"test\": datasets.Dataset.from_pandas(pd.DataFrame(test_data), split=\"test\")\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e435f1422ccd41d9a631e9019157ff08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\huggingface_hub\\hf_api.py:2165: FutureWarning: `identical_ok` has no effect and is deprecated. It will be removed in 0.11.0.\n",
      "  warnings.warn(\n",
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d49d6ec12c44e19e9d8baac817ad54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(\n",
    "    repo_id=\"dougtrajano/olid-br\",\n",
    "    private=True,\n",
    "    token=args.HUGGINGFACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
